{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\12027\\Anaconda3\\envs\\arob\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\12027\\Anaconda3\\envs\\arob\\lib\\tkinter\\__init__.py\", line 804, in callit\n",
      "    func(*args)\n",
      "  File \"<ipython-input-76-39f180b9af5e>\", line 152, in update\n",
      "    s, r, done = env.step(a)\n",
      "  File \"<ipython-input-76-39f180b9af5e>\", line 113, in step\n",
      "    s = self.canvas.coords(self.agent)\n",
      "  File \"C:\\Users\\12027\\Anaconda3\\envs\\arob\\lib\\tkinter\\__init__.py\", line 2761, in coords\n",
      "    self.tk.call((self._w, 'coords') + args))]\n",
      "_tkinter.TclError: invalid command name \".!canvas\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "UNIT = 40  \n",
    "MAZE_H = 10\n",
    "MAZE_W =  10\n",
    "origin = np.array([UNIT/2, UNIT/2])\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self, agentXY, goalXY, walls=[],pits=[]):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.wallblocks = []\n",
    "        self.pitblocks=[]\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_W * UNIT))\n",
    "        self.build_shape_maze(agentXY, goalXY, walls, pits)\n",
    "\n",
    "    def build_shape_maze(self,agentXY,goalXY, walls,pits):\n",
    "        self.canvas = tk.Canvas(self, bg='papayawhip',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)  \n",
    "\n",
    "\n",
    "        for x,y in walls:\n",
    "            self.add_wall(x,y)\n",
    "        for x,y in pits:\n",
    "            self.add_pit(x,y)\n",
    "        self.add_goal(goalXY[0],goalXY[1])\n",
    "        self.add_agent(agentXY[0],agentXY[1])\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def add_wall(self, x, y):\n",
    "        wall_center = origin + np.array([UNIT * x, UNIT*y])\n",
    "        self.wallblocks.append(self.canvas.create_rectangle(\n",
    "            wall_center[0] - 20, wall_center[1] - 20,\n",
    "            wall_center[0] + 20, wall_center[1] + 20,\n",
    "            fill='grey'))\n",
    "        \n",
    "        \n",
    "\n",
    "    '''Add a solid pit block at coordinate for centre of bloc'''\n",
    "    def add_pit(self, x, y):\n",
    "        pit_center = origin + np.array([UNIT * x, UNIT*y])\n",
    "        self.pitblocks.append(self.canvas.create_rectangle(\n",
    "            pit_center[0] - 20, pit_center[1] - 20,\n",
    "            pit_center[0] + 20, pit_center[1] + 20,\n",
    "            fill='yellow2'))\n",
    "\n",
    "    '''Add a solid goal for goal at coordinate for centre of bloc'''\n",
    "    def add_goal(self, x=4, y=4):\n",
    "        goal_center = origin + np.array([UNIT * x, UNIT*y])\n",
    "\n",
    "        self.goal = self.canvas.create_oval(\n",
    "            goal_center[0] - 20, goal_center[1] - 20,\n",
    "            goal_center[0] + 20, goal_center[1] + 20,\n",
    "            fill='red')\n",
    "\n",
    "    def add_agent(self, x=0, y=0):\n",
    "        agent_center = origin + np.array([UNIT * x, UNIT*y])\n",
    "\n",
    "        self.agent = self.canvas.create_rectangle(\n",
    "            agent_center[0] - 20, agent_center[1] - 20,\n",
    "            agent_center[0] + 20, agent_center[1] + 20,\n",
    "            fill='salmon')\n",
    "\n",
    "    def reset(self, value = 1, resetAgent=True):\n",
    "        self.update()\n",
    "        time.sleep(0.2)\n",
    "        if(value == 0):\n",
    "            return self.canvas.coords(self.agent)\n",
    "        else:\n",
    "            #Reset Agent\n",
    "            if(resetAgent):\n",
    "                self.canvas.delete(self.agent)\n",
    "                self.agent = self.canvas.create_rectangle(origin[0] - 20, origin[1] - 20,\n",
    "                origin[0] + 20, origin[1] + 20,\n",
    "                fill='salmon')\n",
    "\n",
    "            return self.canvas.coords(self.agent)\n",
    "\n",
    "    def computeReward(self, currstate, action, nextstate):\n",
    "            reverse=False\n",
    "            if nextstate == self.canvas.coords(self.goal):\n",
    "                reward = 50\n",
    "                done = True\n",
    "                nextstate = 'terminal'\n",
    "            \n",
    "            elif nextstate in [self.canvas.coords(w) for w in self.wallblocks]:\n",
    "                reward = -1\n",
    "                done = False\n",
    "                nextstate = currstate\n",
    "                reverse=True\n",
    "            \n",
    "            elif nextstate in [self.canvas.coords(w) for w in self.pitblocks]:\n",
    "                reward = -10\n",
    "                done = True\n",
    "                nextstate = 'terminal'\n",
    "                reverse=False\n",
    "                \n",
    "            else:\n",
    "                reward = -0.1\n",
    "                done = False\n",
    "            return reward,done, reverse\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.agent)\n",
    "       \n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.agent, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        s_ = self.canvas.coords(self.agent)  \n",
    "        # call the reward function\n",
    "        reward, done, reverse = self.computeReward(s, action, s_)\n",
    "        if(reverse):\n",
    "            self.canvas.move(self.agent, -base_action[0], -base_action[1])  # move agent back\n",
    "            s_ = self.canvas.coords(self.agent) \n",
    "      \n",
    "\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self, sim_speed=.1):\n",
    "        time.sleep(sim_speed)\n",
    "        self.update()\n",
    "\n",
    "\n",
    "def update():\n",
    "    for t in range(10):\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            a =2\n",
    "            s, r, done = env.step(a)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    agentXY=[0,0]\n",
    "    goalXY=[4,5]\n",
    "\n",
    "    \n",
    "    ## Feel free to change the obstacle space and observe the learning\n",
    "        \n",
    "    wall_shape=np.array([[5,2],[4,2],[3,2],[3,3],[3,4],[3,5],[3,6],[4,6],[5,6],[5,3],[5,5]])\n",
    "\n",
    "    pits=np.array([[0,7],[0,6],[0,5],[0,4],[0,3],[0,2],[0,1],[0,8],[0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9],[8,9],[9,8],[9,3],[9,5],[9,0],[8,0],[9,1],[9,2],[9,3],[9,4],[9,5],[9,6],[9,7]])\n",
    "\n",
    "    env = Maze(agentXY,goalXY,wall_shape, pits)\n",
    "\n",
    "    \n",
    "    env.after(100, update)\n",
    "    env.mainloop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_state_exist( state, q_table):\n",
    "\n",
    "    actions = ['u','d','l','r']\n",
    "    if state not in q_table.index:\n",
    "        # append new state to q table\n",
    "        q_table =q_table.append(\n",
    "            pd.Series(\n",
    "                [0]*len(actions),\n",
    "                index=q_table.columns,\n",
    "                name=state,))\n",
    "        \n",
    "        \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over, epoch : 0/4000\n",
      "Game over, epoch : 1/4000\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-d1e4bfff4b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magentXY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgoalXY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwall_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-d1e4bfff4b54>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_val_fut_reward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_val_fut_reward\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_val_fut_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-39f180b9af5e>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mbase_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\arob\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2759\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2760\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2761\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2763\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "\n",
    "###### My_Code ##########\n",
    "def update(env):\n",
    "    \n",
    "    \n",
    "    actions = list(range(4))\n",
    "    reward_episodes_list = []\n",
    "    rewards_list =()\n",
    "    q_table = pd.DataFrame(columns=actions, dtype=np.float64)\n",
    "    \n",
    "    learning_rate=0.7\n",
    "    gamma=0.9\n",
    "    epsilon = 0.8\n",
    "\n",
    "    episode = 4000\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(0, episode):\n",
    "        \n",
    "\n",
    "        #Select a random state\n",
    "#         env =Maze(agentXY,goal_generator(),wall_shape, pits)\n",
    "        int_state = env.reset()\n",
    "\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "\n",
    "            env.render()\n",
    "            #intialize the action for the state\n",
    "\n",
    "            q_table = check_state_exist(str(int_state),q_table)\n",
    "#             print('q_tab1', q_table)\n",
    "\n",
    "            if np.random.uniform() > epsilon:\n",
    "\n",
    "                action = np.random.choice(actions)\n",
    "\n",
    "            else:\n",
    "\n",
    "                max_val_fut_reward = q_table.loc[str(int_state), :]\n",
    "                action = np.random.choice(max_val_fut_reward[max_val_fut_reward==np.max(max_val_fut_reward)].index)\n",
    "\n",
    "            new_state, reward, done = env.step(action)          \n",
    "            episode_reward += reward\n",
    "\n",
    "            q_table = check_state_exist(str(new_state),q_table)\n",
    "#             print('q_tab2', q_table)\n",
    "\n",
    "            q_predict = q_table.loc[str(int_state), action]\n",
    "\n",
    "            if new_state != 'terminal':\n",
    "\n",
    "    #             current_q = q_table.loc[str(int_state), action]\n",
    "                  max_exp_reward = np.max(q_table.loc[str(new_state), :])\n",
    "\n",
    "                  q_target =( reward + gamma* max_exp_reward)\n",
    "\n",
    "\n",
    "            else:            \n",
    "                #Update the Q_tabel using the bellmans equation\n",
    "                #New Q value = Current Q value + lr * [Reward + discount_rate * (highest Q value between possible actions from the new state s’ ) — Current Q value ]\n",
    "\n",
    "    #             current_q = q_table.loc[str(int_state), action]\n",
    "\n",
    "                q_target= reward\n",
    "        \n",
    "\n",
    "\n",
    "            q_table.loc[str(int_state), action] += learning_rate * ( q_target - q_predict)\n",
    "            int_state = new_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print('Game over, epoch : {}/{}'.format(epoch, episode))\n",
    "        reward_episodes_list.append([episode_reward, epoch])\n",
    "        \n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel(' Reward obtained')\n",
    "    plt.title('Reward obtained for the episode')\n",
    "    print(reward_episodes_list)   \n",
    "    q_table.head()\n",
    "    \n",
    "    plt.plot(np.asarray(reward_episodes_list)[:,1],np.asarray(reward_episodes_list)[:,0])\n",
    "    \n",
    "    \n",
    "    env.destroy()\n",
    "    \n",
    "# Convert the Q table to CSV and save, Uncomment if you are generating for a grid with new obstacles\n",
    "#     q_table.to_csv(r'C:\\Users\\12027\\PyCharm_Projects\\Untitled Folder\\Q_TABLE_10.csv')\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = Maze(agentXY,goalXY,wall_shape, pits)\n",
    "    \n",
    "    env.after(100, update(env))\n",
    "\n",
    "    env.mainloop()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################### Epsilon_Greedy approach for choosing action #############\n",
    "\n",
    "def update(env):\n",
    "    \n",
    "    \n",
    "    actions = list(range(4))\n",
    "    reward_episodes_list = []\n",
    "    rewards_list =()\n",
    "    q_table = pd.DataFrame(columns=actions, dtype=np.float64)\n",
    "    \n",
    "    learning_rate=0.6\n",
    "    gamma=0.9\n",
    "\n",
    "    episode = 100\n",
    "\n",
    "    epsilon = 1  # not a constant, qoing to be decayed\n",
    "    START_EPSILON_DECAYING = 1\n",
    "    END_EPSILON_DECAYING = episode//2\n",
    "    epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(0, episode):\n",
    "        \n",
    "\n",
    "        #Select a random state\n",
    "#         env =Maze(agentXY,goal_generator(),wall_shape, pits)\n",
    "        int_state = env.reset()\n",
    "\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "\n",
    "            env.render()\n",
    "            #intialize the action for the state\n",
    "\n",
    "            q_table = check_state_exist(str(int_state),q_table)\n",
    "#             print('q_tab1', q_table)\n",
    "\n",
    "            if np.random.uniform() > epsilon:\n",
    "\n",
    "                action = np.random.choice(actions)\n",
    "\n",
    "            else:\n",
    "\n",
    "                max_val_fut_reward = q_table.loc[str(int_state), :]\n",
    "                action = np.random.choice(max_val_fut_reward[max_val_fut_reward==np.max(max_val_fut_reward)].index)\n",
    "\n",
    "            new_state, reward, done = env.step(action)          \n",
    "            episode_reward += reward\n",
    "\n",
    "            q_table = check_state_exist(str(new_state),q_table)\n",
    "#             print('q_tab2', q_table)\n",
    "\n",
    "            q_predict = q_table.loc[str(int_state), action]\n",
    "\n",
    "            if new_state != 'terminal':\n",
    "            \n",
    "                \n",
    "            \n",
    "\n",
    "    #             current_q = q_table.loc[str(int_state), action]\n",
    "                  max_exp_reward = np.max(q_table.loc[str(new_state), :])\n",
    "        \n",
    "                  current_q = q_table.loc[str(int_state), action]\n",
    "            \n",
    "                  new_q = (1 - learning_rate) * current_q + learning_rate * ( reward + gamma * max_exp_reward)\n",
    "                \n",
    "                \n",
    "                  current_q = new_q \n",
    "\n",
    "                  \n",
    "\n",
    "\n",
    "            else:            \n",
    "                #Update the Q_tabel using the bellmans equation\n",
    "                #New Q value = Current Q value + lr * [Reward + discount_rate * (highest Q value between possible actions from the new state s’ ) — Current Q value ]\n",
    "\n",
    "                current_q = q_table.loc[str(int_state), action]\n",
    "\n",
    "                current_q = reward\n",
    "        \n",
    "\n",
    "\n",
    "#             q_table.loc[str(int_state), action] += learning_rate * ( q_target - q_predict)\n",
    "            int_state = new_state\n",
    "            \n",
    "            if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "                \n",
    "                epsilon -= epsilon_decay_value\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print('Game over, epoch : {}/{}'.format(epoch, episode))\n",
    "        reward_episodes_list.append([episode_reward, epoch])\n",
    "   \n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel(' Reward obtained')\n",
    "    plt.title('Reward obtained for the episode')\n",
    "    print(reward_episodes_list)   \n",
    "    \n",
    "    plt.plot(np.asarray(reward_episodes_list)[:,1],np.asarray(reward_episodes_list)[:,0])\n",
    "    \n",
    "    \n",
    "    env.destroy()\n",
    "#     q_table.to_csv(r'C:\\Users\\12027\\PyCharm_Projects\\Untitled Folder\\Q_TABLE_4.csv')\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    env = Maze(agentXY,goalXY,wall_shape, pits)\n",
    "    \n",
    "    env.after(100, update(env))\n",
    "\n",
    "    env.mainloop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
